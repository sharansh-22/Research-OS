# Research-OS - Complete Project Structure & Architecture

## ğŸ“‹ Executive Summary

**Research-OS** is an end-to-end Retrieval-Augmented Generation (RAG) system that enables intelligent querying over a multi-format knowledge base. It combines dense vector search (FAISS), sparse keyword search (BM25), cross-encoder reranking (FlashRank), and LLM inference (Groq API primary, Ollama fallback) to provide accurate, context-aware answers with source attribution.

### Key Features
- **Hybrid Search**: FAISS (semantic) + BM25 (keyword) with Reciprocal Rank Fusion (RRF)
- **Cross-Encoder Reranking**: FlashRank (ms-marco-TinyBERT-L-2-v2) for precision
- **Multi-Format Ingestion**: PDF, Python, Jupyter, Markdown, LaTeX, C++/CUDA
- **Smart Query Routing**: Intent classification (code / theory / hybrid)
- **Streaming Generation**: Server-Sent Events (SSE) for real-time token delivery
- **3-Turn Conversation Memory**: Sliding window context for multi-turn chat
- **React Frontend**: Three-pane UI with session management and source inspector
- **FastAPI Backend**: RESTful API with auth, background ingestion, and health checks
- **System Integrity**: MD5 file hashing, ingestion ledger, constant-time API key comparison

---

## ğŸ“ Directory Structure

```
Research-OS/
â”‚
â”œâ”€â”€ ğŸ“„ ROOT CONFIGURATION FILES
â”‚   â”œâ”€â”€ .gitattributes              # Git LFS config for PDFs/binary files
â”‚   â”œâ”€â”€ .gitignore                  # Git ignore patterns
â”‚   â”œâ”€â”€ requirements.txt            # Python package dependencies (pinned)
â”‚   â”œâ”€â”€ local.env                   # Local environment overrides
â”‚   â””â”€â”€ .env                        # Environment variables (GROQ_API_KEY, RESEARCH_OS_API_KEY, etc.)
â”‚
â”œâ”€â”€ ğŸ“„ ENTRY POINTS
â”‚   â”œâ”€â”€ main.py                     # CLI application (interactive REPL + --ingest + --query)
â”‚   â”œâ”€â”€ run_api.py                  # FastAPI server launcher (uvicorn)
â”‚   â””â”€â”€ verify_setup.py             # 22-section system verification (pre-flight checker)
â”‚
â”œâ”€â”€ ğŸ“ src/                         # MAIN SOURCE CODE PACKAGE
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ rag/                     # RAG Core Module
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ __init__.py             # Package exports (18 classes, v2.1.0)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ data_loader.py          # UNIVERSAL DOCUMENT LOADER
â”‚   â”‚   â”‚   â”œâ”€â”€ UniversalLoader     # Factory: routes files to format-specific parsers
â”‚   â”‚   â”‚   â”œâ”€â”€ PDFParser           # pymupdf4llm â†’ markdown â†’ section splitting
â”‚   â”‚   â”‚   â”œâ”€â”€ PythonParser        # Split by functions/classes
â”‚   â”‚   â”‚   â”œâ”€â”€ JupyterParser       # Code/markdown cells
â”‚   â”‚   â”‚   â”œâ”€â”€ MarkdownParser      # Split by headers
â”‚   â”‚   â”‚   â”œâ”€â”€ LaTeXParser         # Split by \section, \subsection
â”‚   â”‚   â”‚   â”œâ”€â”€ CppParser           # Brace-matched function extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ TextParser          # Fallback plain text
â”‚   â”‚   â”‚   â”œâ”€â”€ Chunk               # Dataclass: content + ChunkType + metadata
â”‚   â”‚   â”‚   â””â”€â”€ ChunkType           # Enum: code | theory | algorithm | theorem | proof | definition
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ embedder.py             # TEXT EMBEDDING (FastEmbed / ONNX)
â”‚   â”‚   â”‚   â”œâ”€â”€ FastEmbedder        # Wraps fastembed.TextEmbedding
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Model: sentence-transformers/all-MiniLM-L6-v2
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Dimension: 384
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ embed()         # Batch embed with L2 normalization
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ embed_query()   # Single query embedding
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ embed_documents()  # Batch document embedding
â”‚   â”‚   â”‚   â””â”€â”€ get_embedder()      # Singleton accessor
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ retriever.py            # HYBRID SEARCH (FAISS + BM25 + FlashRank)
â”‚   â”‚   â”‚   â”œâ”€â”€ HybridRetriever
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FAISS IndexFlatIP (inner product, 384-dim)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BM25Okapi sparse index
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FlashRank cross-encoder reranker
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ search()                     # Hybrid search with RRF merge
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ search_by_type_filtered()    # Intent-based pre-filtering
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ _reciprocal_rank_fusion()    # RRF (k=60, FAISS: 0.7, BM25: 0.3)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ save() / load()              # Full persistence (faiss.index, chunks.pkl, bm25.pkl)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ add_documents()              # Add chunks to both indices
â”‚   â”‚   â”‚   â””â”€â”€ RetrievalResult     # Dataclass: chunk + score + rank + source
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ generator.py            # LLM ANSWER GENERATION
â”‚   â”‚   â”‚   â”œâ”€â”€ ResearchArchitect
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Primary: Groq API (llama-3.3-70b-versatile)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Fallback: Ollama (phi3:mini)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MAX_HISTORY_TURNS = 3 (sliding window)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generate_stream()    # Streaming token generation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generate()           # Non-streaming generation
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ health_check()       # Backend connectivity test
â”‚   â”‚   â”‚   â””â”€â”€ GenerationResult    # Dataclass: response + metadata
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ pipeline.py             # RAG PIPELINE ORCHESTRATION
â”‚   â”‚   â”‚   â”œâ”€â”€ ResearchPipeline
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ classify_intent()    # Smart query routing (code/theory/hybrid)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ query()              # Full RAG: retrieve â†’ generate â†’ cite
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ query_stream()       # Streaming RAG with JSON-serializable chunks
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ingest_pdf()         # Single file ingestion with MD5 dedup
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ingest_directory()   # Batch directory ingestion
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rebuild_index()      # Full reindex from scratch
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ save_index() / load_index()  # Persistence
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ get_stats()          # Index statistics
â”‚   â”‚   â”‚   â”œâ”€â”€ PipelineConfig      # Dataclass: index_dir, enable_fallback, etc.
â”‚   â”‚   â”‚   â”œâ”€â”€ QueryResult         # Dataclass: response + intent + context + metadata
â”‚   â”‚   â”‚   â”œâ”€â”€ IngestionResult     # Dataclass: filename + status + chunks_added + hash
â”‚   â”‚   â”‚   â”œâ”€â”€ StreamChunk         # Dataclass: event + data (for SSE)
â”‚   â”‚   â”‚   â””â”€â”€ create_pipeline()   # Factory function
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ verifier.py             # CODE VERIFICATION SANDBOX
â”‚   â”‚       â”œâ”€â”€ ArchitectureVerifier
â”‚   â”‚       â”‚   â”œâ”€â”€ verify_dimensions()          # Execute code, extract tensor shapes
â”‚   â”‚       â”‚   â”œâ”€â”€ verify_generated_response()  # Verify all code blocks in LLM output
â”‚   â”‚       â”‚   â”œâ”€â”€ extract_code_blocks()        # Parse ```python``` fences
â”‚   â”‚       â”‚   â”œâ”€â”€ _is_safe()                   # Regex safety check (blocks os, subprocess, eval, exec)
â”‚   â”‚       â”‚   â””â”€â”€ Timeout: SIGALRM-based (10s default)
â”‚   â”‚       â””â”€â”€ VerificationResult  # Dataclass: success + output + shapes + execution_time
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ api/                     # FastAPI Backend Module
â”‚   â”‚   â”œâ”€â”€ __init__.py             # Exports create_app()
â”‚   â”‚   â”œâ”€â”€ main.py                 # App factory + lifespan (startup/shutdown)
â”‚   â”‚   â”‚   â”œâ”€â”€ create_app()        # FastAPI instance with CORS + routes
â”‚   â”‚   â”‚   â””â”€â”€ CORS origins: localhost:5173, 5174, 3000
â”‚   â”‚   â”œâ”€â”€ routes.py               # HTTP Endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ POST /v1/chat       # Streaming chat (SSE) with RAG
â”‚   â”‚   â”‚   â”œâ”€â”€ POST /v1/ingest/file    # Multipart file upload â†’ background ingest
â”‚   â”‚   â”‚   â”œâ”€â”€ POST /v1/ingest/url     # URL download â†’ background ingest
â”‚   â”‚   â”‚   â”œâ”€â”€ GET  /v1/ingest/status  # Ingestion task progress
â”‚   â”‚   â”‚   â”œâ”€â”€ GET  /v1/index/files    # List indexed documents
â”‚   â”‚   â”‚   â””â”€â”€ GET  /health            # System health + backend status
â”‚   â”‚   â”œâ”€â”€ dependencies.py         # Security & Singleton
â”‚   â”‚   â”‚   â”œâ”€â”€ PipelineState       # Global RAG pipeline singleton (lifespan-managed)
â”‚   â”‚   â”‚   â””â”€â”€ verify_api_key()    # X-API-Key header â†’ hmac.compare_digest (constant-time)
â”‚   â”‚   â””â”€â”€ ingestion_tracker.py    # Background task progress tracking
â”‚   â”‚       â”œâ”€â”€ IngestionStage      # Enum: DOWNLOADING, PARSING, EMBEDDING, INDEXING, COMPLETE, FAILED
â”‚   â”‚       â””â”€â”€ tracker             # Global tracker instance
â”‚   â”‚
â”‚   â””â”€â”€ api.py                      # Legacy monolithic API (superseded by src/api/)
â”‚
â”œâ”€â”€ ğŸ“ scripts/                     # UTILITY SCRIPTS
â”‚   â”œâ”€â”€ download_data.py            # Download knowledge base PDFs
â”‚   â”œâ”€â”€ auto_download.py            # Auto-download + classify documents
â”‚   â”œâ”€â”€ ingest_batch.py             # Batch PDF ingestion
â”‚   â”œâ”€â”€ verify_setup.py             # Duplicate of root verify_setup.py
â”‚   â”œâ”€â”€ check_metadata.py           # Index metadata inspector
â”‚   â”œâ”€â”€ diagnose_index.py           # Index diagnostics
â”‚   â””â”€â”€ Analyze-logs.py             # Query log analysis
â”‚
â”œâ”€â”€ ğŸ“ frontend/                    # REACT FRONTEND (Vite + Tailwind v3)
â”‚   â”œâ”€â”€ package.json                # npm config (React 19, Tailwind 3)
â”‚   â”œâ”€â”€ vite.config.js              # Vite build config
â”‚   â”œâ”€â”€ tailwind.config.js          # Tailwind theme (custom dark palette)
â”‚   â”œâ”€â”€ postcss.config.js           # PostCSS config
â”‚   â”œâ”€â”€ index.html                  # HTML entry point
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.jsx                # React entry point
â”‚       â”œâ”€â”€ App.jsx                 # Three-pane layout + session management
â”‚       â”‚   â”œâ”€â”€ MAX_HISTORY_TURNS = 3 (mirrored from backend)
â”‚       â”‚   â”œâ”€â”€ pushHistory()       # Sliding window history manager
â”‚       â”‚   â””â”€â”€ Health polling (30s interval)
â”‚       â”œâ”€â”€ api.js                  # API client
â”‚       â”‚   â”œâ”€â”€ streamChat()        # SSE streaming via fetch + ReadableStream
â”‚       â”‚   â”œâ”€â”€ uploadFile()        # Multipart file upload
â”‚       â”‚   â”œâ”€â”€ ingestUrl()         # URL ingestion
â”‚       â”‚   â”œâ”€â”€ fetchHealth()       # Health check
â”‚       â”‚   â””â”€â”€ API key via localStorage (X-API-Key header)
â”‚       â”œâ”€â”€ chatHistory.js          # Session persistence (localStorage, 50-session cap)
â”‚       â”œâ”€â”€ index.css               # Tailwind styles + custom theme
â”‚       â””â”€â”€ components/
â”‚           â”œâ”€â”€ ChatPane.jsx        # Chat interface with streaming
â”‚           â”œâ”€â”€ MessageBubble.jsx   # Markdown renderer (react-markdown, KaTeX, highlight.js)
â”‚           â”œâ”€â”€ LeftPane.jsx        # Sidebar: sessions + file upload + URL ingest
â”‚           â”œâ”€â”€ SourcePane.jsx      # Source inspector (right pane)
â”‚           â”œâ”€â”€ ApiKeyModal.jsx     # API key configuration modal
â”‚           â””â”€â”€ ChatHistoryPanel.jsx  # Chat history UI
â”‚
â”œâ”€â”€ ğŸ“ backend/                     # BACKEND MODELS
â”‚   â””â”€â”€ models/                     # ML model storage
â”‚
â”œâ”€â”€ ğŸ“ data/                        # DATA & KNOWLEDGE BASE
â”‚   â”œâ”€â”€ ğŸ“ 01_fundamentals/         # Fundamental ML resources
â”‚   â”œâ”€â”€ ğŸ“ 02_papers/               # Research papers
â”‚   â”œâ”€â”€ ğŸ“ 03_implementation/       # Implementation guides
â”‚   â”œâ”€â”€ ğŸ“ 04_misc/                 # Uploaded / miscellaneous files
â”‚   â””â”€â”€ ğŸ“ index/                   # PERSISTED SEARCH INDICES
â”‚       â”œâ”€â”€ faiss.index             # FAISS vector database (384-dim, IndexFlatIP)
â”‚       â”œâ”€â”€ chunks.pkl              # Chunk objects (pickle)
â”‚       â”œâ”€â”€ chunk_texts.pkl         # Raw text for BM25
â”‚       â”œâ”€â”€ bm25.pkl                # BM25 sparse index (pickle)
â”‚       â”œâ”€â”€ config.json             # Index config (n_chunks, dimension, model)
â”‚       â””â”€â”€ processed_files.json    # Ingestion ledger (filename â†’ MD5 hash + chunks + timestamp)
â”‚
â”œâ”€â”€ ğŸ“ tests/                       # UNIT & INTEGRATION TESTS
â”‚   â”œâ”€â”€ test_data_loader.py         # Chunking, metadata, format detection
â”‚   â””â”€â”€ test_hybrid_search.py       # Index persistence, hybrid ranking
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                   # JUPYTER NOTEBOOKS
â”‚   â”œâ”€â”€ 1-text-extraction.ipynb     # PDF extraction experiments
â”‚   â””â”€â”€ 2-embedding.ipynb           # Embedding experiments
â”‚
â”œâ”€â”€ ğŸ“ logs/                        # QUERY LOGS & ANALYTICS
â”‚
â”œâ”€â”€ ğŸ“ .cache/                      # MODEL CACHES
â”‚   â””â”€â”€ flashrank/                  # FlashRank reranker model
â”‚       â””â”€â”€ ms-marco-TinyBERT-L-2-v2/
â”‚           â”œâ”€â”€ flashrank-TinyBERT-L-2-v2.onnx
â”‚           â”œâ”€â”€ tokenizer.json
â”‚           â””â”€â”€ config.json
â”‚
â”œâ”€â”€ .vscode/                        # IDE SETTINGS
â”‚   â””â”€â”€ settings.json               # Python interpreter + conda auto-activation
â”‚
â””â”€â”€ .github/                        # GitHub workflows
```

---

## ğŸ”— Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          USER INTERACTION                                     â”‚
â”‚  CLI (main.py) â”‚ React Frontend (port 5173) â”‚ API Docs (/docs)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                  â”‚                               â”‚
             â”‚           FastAPI Backend (port 8000)            â”‚
             â”‚           X-API-Key auth + SSE streaming         â”‚
             â”‚                  â”‚                               â”‚
       â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ INGESTION   â”‚    â”‚   QUERY    â”‚              â”‚  HEALTH/STATS   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                  â”‚
             â–¼                  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ UniversalLoader    â”‚  â”‚ classify_intent(query)   â”‚
  â”‚ (PDF/LaTeX/C++/    â”‚  â”‚ â†’ code | theory | hybrid â”‚
  â”‚  Python/Jupyter/   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚  Markdown)         â”‚             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â–¼
           â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼              â”‚ HybridRetriever          â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”œâ”€ FAISS (semantic)     â”‚
  â”‚ FastEmbedder       â”‚  â”‚  â”œâ”€ BM25  (keyword)      â”‚
  â”‚ (all-MiniLM-L6-v2) â”‚  â”‚  â”œâ”€ RRF merge (k=60)    â”‚
  â”‚ â†’ 384-dim vectors  â”‚  â”‚  â””â”€ FlashRank rerank     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚
           â–¼                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Add to FAISS +     â”‚  â”‚ ResearchArchitect        â”‚
  â”‚ BM25 indices       â”‚  â”‚  â”œâ”€ Groq (primary)       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”œâ”€ Ollama (fallback)    â”‚
           â”‚              â”‚  â””â”€ 3-turn memory window  â”‚
           â–¼              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
  â”‚ Save to data/index â”‚             â–¼
  â”‚ + update ledger    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ (MD5 hash tracking)â”‚  â”‚ SSE Stream â†’ Frontend    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ (start â†’ context â†’       â”‚
                          â”‚  chunks â†’ sources â†’ done) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Component Interaction Details

### 1. **Data Ingestion Pipeline**
```
Document (PDF / .py / .tex / .cpp / .ipynb / .md)
  â†“ UniversalLoader.load_file()
  â”œâ”€ Route to format-specific parser (PDFParser, CppParser, etc.)
  â”œâ”€ Extract text â†’ create Chunk objects with ChunkType + metadata
  â””â”€ Return: List[Chunk]
    â†“ ResearchPipeline.ingest_pdf()
    â”œâ”€ Compute MD5 hash â†’ check ledger for duplicates
    â”œâ”€ Embed chunks via FastEmbedder (384-dim, L2-normalized)
    â””â”€ Add to HybridRetriever (FAISS + BM25)
      â†“ save_index()
      â”œâ”€ Persist: data/index/faiss.index, chunks.pkl, bm25.pkl
      â”œâ”€ Write: data/index/config.json
      â””â”€ Update: data/index/processed_files.json (ledger)
```

### 2. **Query & Retrieval Pipeline**
```
User Query (string)
  â†“ ResearchPipeline.query(question, history, filter_type)
  â”œâ”€ classify_intent(query)  â†’  code | theory | hybrid
  â”œâ”€ HybridRetriever.search_by_type_filtered(query, top_k, intent)
  â”‚  â”œâ”€ FAISS search: inner product â†’ top-k semantic matches
  â”‚  â”œâ”€ BM25 search: Okapi BM25 â†’ top-k keyword matches
  â”‚  â”œâ”€ Reciprocal Rank Fusion (k=60, FAISS: 0.7, BM25: 0.3)
  â”‚  â””â”€ FlashRank cross-encoder reranking
  â””â”€ Return: List[RetrievalResult] (sorted by score)
```

### 3. **Answer Generation Pipeline**
```
(Query, Retrieved Chunks, History)
  â†“ ResearchArchitect.generate_stream()
  â”œâ”€ Build conversation: system prompt + history[-6:] + context + query
  â”œâ”€ Primary: Groq API (llama-3.3-70b-versatile)
  â”‚  â””â”€ Fallback: Ollama (phi3:mini) if Groq fails
  â”œâ”€ Stream tokens via SSE events
  â””â”€ Post-process: strip hallucinated sources, inject metadata-based citations
    â†“ Deliver via EventSourceResponse to frontend
```

---

## ğŸ“¦ Dependencies & Requirements

### Core RAG Packages
```
pymupdf4llm        # PDF â†’ markdown extraction
pymupdf (fitz)     # PDF parsing engine
fastembed           # ONNX-based text embeddings (all-MiniLM-L6-v2)
faiss-cpu           # Vector similarity search (CPU-only)
rank-bm25           # BM25 keyword ranking
flashrank           # Cross-encoder reranking (TinyBERT)
numpy (<2.0.0)      # Numerical computations
groq                # Groq cloud LLM API client
ollama              # Local LLM fallback client
pydantic            # Data validation
python-dotenv       # Environment variable management
```

### API Packages
```
fastapi             # Web framework
uvicorn             # ASGI server
sse-starlette       # Server-Sent Events support
python-multipart    # File upload handling
```

### Frontend Packages (npm)
```
react (^19.2.0)     # UI framework
react-markdown      # Markdown rendering
remark-math         # Math notation parsing
rehype-katex        # KaTeX rendering
katex               # Math typesetting
highlight.js        # Code syntax highlighting
lucide-react        # Icons
tailwindcss (^3.4)  # Utility-first CSS (v3)
```

### Development Dependencies
```
pytest              # Unit testing
pytest-asyncio      # Async test support
jupyter             # Interactive notebooks
httpx               # HTTP testing client
```

### System Requirements
- **Python**: 3.10+ (tested on 3.10)
- **RAM**: 16 GB (optimized for FAISS + BM25 + embedder in-memory)
- **GPU**: Not required (ONNX CPU inference via FastEmbed)
- **Ollama**: Optional fallback â€” `ollama serve` on localhost:11434
- **Groq API Key**: Required for primary LLM generation
- **Node.js**: 18+ (for frontend)

---

## ğŸš€ Usage Workflows

### Setup & Installation
```bash
# Clone repository
git clone <repo-url>
cd Research-OS

# Create conda environment
conda create -n Research-OS python=3.10
conda activate Research-OS

# Install Python dependencies
pip install -r requirements.txt

# Install frontend dependencies
cd frontend && npm install && cd ..

# Set environment variables
cp .env.example .env   # Edit with your GROQ_API_KEY

# Verify everything
python verify_setup.py

# Download knowledge base PDFs
python scripts/download_data.py

# Ingest PDFs into index
python scripts/ingest_batch.py
```

### Usage Modes

#### 1. Interactive CLI Mode
```bash
python main.py
```

#### 2. API Server + Frontend
```bash
# Terminal 1: Start API
python run_api.py

# Terminal 2: Start frontend
cd frontend && npm run dev
```

#### 3. Single Query (CLI)
```bash
python main.py --query "What is the attention mechanism?"
```

#### 4. Batch Ingestion
```bash
python scripts/ingest_batch.py
```

#### 5. System Verification
```bash
python verify_setup.py
```

#### 6. Testing
```bash
pytest tests/
```

---

## ğŸ”§ Configuration & Extensibility

### Environment Variables (.env)
```bash
GROQ_API_KEY=gsk_...                  # Groq API key (required for primary LLM)
RESEARCH_OS_API_KEY=...               # API authentication key
API_URL=http://localhost:8000         # Backend URL
RESEARCH_OS_INDEX_DIR=data/index      # Index storage directory
RESEARCH_OS_CORS_ORIGINS=*            # CORS policy (overridden in src/api/main.py)
```

### Key Configuration Points

**Change LLM Model** (generator.py):
```python
MODEL = "llama-3.3-70b-versatile"     # Groq primary
FALLBACK_MODEL = "phi3:mini"          # Ollama fallback
```

**Adjust Hybrid Search Weights** (retriever.py):
```python
self.faiss_weight = 0.7               # Semantic search weight
self.bm25_weight = 0.3                # Keyword search weight
```

**Change Embedding Model** (embedder.py):
```python
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
EMBEDDING_DIM = 384
```

**Adjust Memory Window** (generator.py + App.jsx):
```python
MAX_HISTORY_TURNS = 3                 # 3 user+assistant turn pairs = 6 messages
```

---

## ğŸ“Š Performance Characteristics

| Operation | Time | Notes |
|-----------|------|-------|
| PDF Ingestion (10 pages) | 5-10s | pymupdf4llm + embedding |
| Embedding Single Text | ~10ms | FastEmbed ONNX (CPU) |
| FAISS Search (3845 chunks) | ~2ms | Inner product, flat index |
| BM25 Search | ~5ms | Okapi BM25 ranking |
| RRF Merge + FlashRank Rerank | ~20ms | Cross-encoder reranking |
| Full Hybrid Search | ~30ms | FAISS + BM25 + RRF + rerank |
| LLM Generation (Groq) | 1-5s | Cloud API, streaming |
| LLM Generation (Ollama) | 5-30s | Local fallback, CPU |
| Full RAG Query | 2-10s | Retrieve + Generate (Groq) |

---

## ğŸ§ª Testing Strategy

### Unit Tests
- **test_data_loader.py**: Chunking logic, metadata preservation, format detection
- **test_hybrid_search.py**: Index persistence, hybrid ranking, RRF correctness

### System Verification
- **verify_setup.py**: 22-section pre-flight checker (Python, env vars, packages, source files, index, FlashRank, imports, embedder, generator, pipeline, syntax, frontend, network, CORS, security)

### Manual Testing
- Interactive CLI testing
- API endpoint testing via `/docs` (Swagger UI)
- Frontend SSE streaming validation

---

## ğŸ“ Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| "No module named 'faiss'" | FAISS not installed | `pip install faiss-cpu` |
| "GROQ_API_KEY not set" | Missing env var | Set in `.env` or `export GROQ_API_KEY='gsk_...'` |
| "Pipeline not initialized" | API started without index | Run `python scripts/ingest_batch.py` first |
| "CORS error in browser" | Frontend origin not allowed | Check `src/api/main.py` CORS origins |
| "401 Missing API key" | No X-API-Key header | Set API key in frontend settings modal |
| "Index not found" | No ingestion completed | Run `python scripts/ingest_batch.py` |
| "Slow query response" | Using Ollama fallback | Check Groq API key and connectivity |
| "Import errors" | Wrong conda env | `conda activate Research-OS` |

---

## ğŸ“„ License & Attribution

- **LLM**: Meta LLaMA 3.3 (via Groq), Microsoft Phi-3 (via Ollama)
- **Embeddings**: FastEmbed (ONNX) / Sentence Transformers
- **Vector DB**: Meta FAISS
- **PDF Extraction**: pymupdf4llm
- **Reranking**: FlashRank (ms-marco-TinyBERT-L-2-v2)
- **BM25**: Rank-bm25 library
- **Frontend**: React 19, Tailwind CSS 3, Vite

---

**Last Updated**: February 12, 2026
**Version**: 2.1.0
**Maintainers**: Research-OS Team
