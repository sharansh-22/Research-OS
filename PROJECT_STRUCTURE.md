# AI Knowledge Assistant - Complete Project Structure & Architecture

## ğŸ“‹ Executive Summary

The **AI Knowledge Assistant** is an end-to-end Retrieval-Augmented Generation (RAG) system that enables intelligent querying over a knowledge base of PDFs. It combines dense vector search (FAISS), sparse keyword search (BM25), and local LLM inference (Ollama) to provide accurate, context-aware answers with source attribution.

### Key Features
- **Hybrid Search**: FAISS (semantic) + BM25 (keyword) for comprehensive retrieval
- **Smart Text Processing**: Intelligent PDF chunking with metadata preservation
- **Local LLM Generation**: Ollama integration for offline inference
- **Interactive CLI**: Real-time query processing with source tracking
- **Modular Architecture**: Pluggable components for extensibility
- **Batch Ingestion**: Process multiple PDFs efficiently

---

## ğŸ“ Directory Structure

```
ai-knowledge-assistant/
â”‚
â”œâ”€â”€ ğŸ“„ ROOT CONFIGURATION FILES
â”‚   â”œâ”€â”€ .gitattributes              # Git LFS (Large File Storage) config for PDFs/binary files
â”‚   â”œâ”€â”€ .gitignore                  # Git ignore patterns
â”‚   â”‚   â””â”€â”€ Excludes: logs/, .env, __pycache__, .vscode, data/*, etc.
â”‚   â”œâ”€â”€ requirements.txt            # Python package dependencies (see section below)
â”‚   â”œâ”€â”€ local.env                   # Local environment variables (not in version control)
â”‚   â””â”€â”€ .env                        # Environment variables template (Git-tracked)
â”‚
â”œâ”€â”€ ğŸ“„ ENTRY POINT & CLI
â”‚   â”œâ”€â”€ main.py                     # Primary CLI application
â”‚   â”‚   â”œâ”€â”€ print_banner()          # Display application header
â”‚   â”‚   â”œâ”€â”€ main()                  # Parse args, orchestrate pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ --ingest <path>     # Ingest and build index from PDF
â”‚   â”‚   â”‚   â””â”€â”€ --query <text>      # Run single query
â”‚   â”‚   â”œâ”€â”€ answer_query()          # Execute query, display results with sources
â”‚   â”‚   â””â”€â”€ INTERACTIVE MODE        # REPL loop for continuous querying
â”‚   â”‚
â”‚   â”œâ”€â”€ test_gemini.py              # Test suite for Gemini API integration
â”‚   â”‚   â”œâ”€â”€ API connectivity tests
â”‚   â”‚   â”œâ”€â”€ Model inference tests
â”‚   â”‚   â””â”€â”€ Response parsing tests
â”‚   â”‚
â”‚   â””â”€â”€ Analyze-logs.py             # Query log analysis utility
â”‚       â”œâ”€â”€ Load logs/queries.jsonl (query history)
â”‚       â”œâ”€â”€ Analyze query patterns & drift
â”‚       â””â”€â”€ Generate statistics & insights
â”‚
â”œâ”€â”€ ğŸ“ src/                         # MAIN SOURCE CODE PACKAGE
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ rag/                     # RAG (Retrieval-Augmented Generation) Module
â”‚   â”‚   â”‚                           # Purpose: Core RAG functionality
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ __init__.py             # Package initialization (exports key classes)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ data_loader.py          # PDF TEXT EXTRACTION & CHUNKING
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_text_with_metadata()
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: PDF file path
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process: Page-by-page extraction using marker-pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Output: List of {text, metadata{page, source}} dicts
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ split_text_smart(text, chunk_size, overlap_ratio)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: Raw text, target chunk size, overlap ratio
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process: Word-based splitting with overlap preservation
â”‚   â”‚   â”‚   â”‚   â”‚  â””â”€â”€ Respects paragraph boundaries when possible
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Output: List of text chunks with start/end indices
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ load_and_chunk_pdf(pdf_path, chunk_size, overlap_ratio)
â”‚   â”‚   â”‚       â”œâ”€â”€ Input: PDF path, chunking parameters
â”‚   â”‚   â”‚       â”œâ”€â”€ Process: Extract â†’ Split â†’ Attach metadata to each chunk
â”‚   â”‚   â”‚       â””â”€â”€ Output: List of {text, metadata{page, source, chunk_idx}} dicts
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ embedder.py             # TEXT EMBEDDING (SentenceTransformers)
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ Embedder class
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__(model_name: str)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Loads SentenceTransformer model (default: all-MiniLM-L6-v2)
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ encode(texts: List[str]) -> np.ndarray
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: List of text strings
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process: Batch encode using SentenceTransformer
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Output: (N, 384) numpy array of embeddings
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ encode_single(text: str) -> np.ndarray
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ Input: Single text string
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ Process: Encode single text
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ Output: (384,) numpy array embedding
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ Model Details:
â”‚   â”‚   â”‚       â”œâ”€â”€ Embedding Dimension: 384
â”‚   â”‚   â”‚       â”œâ”€â”€ Max Sequence Length: 512 tokens
â”‚   â”‚   â”‚       â””â”€â”€ Inference: CPU or CUDA (auto-detected)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ retriever.py            # HYBRID SEARCH (FAISS + BM25)
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ Retriever class
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__(embedding_dim: int = 384)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Initialize FAISS index (flat, no GPU)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Initialize BM25 ranker
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Initialize document storage
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ add(documents: List[Dict])
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: {text, metadata{page, source, ...}}
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process 1: Extract text & embed with SentenceTransformer
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process 2: Add embeddings to FAISS index
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process 3: Build BM25 corpus from texts
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Store original documents for retrieval
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ search(query: str, k: int = 5) -> List[Dict]
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: Query text, number of results
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process 1: Embed query text
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process 2: Search FAISS for top-k semantic matches
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process 3: Search BM25 for top-k keyword matches
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process 4: Merge & re-rank results (hybrid scoring)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Output: Sorted list of {text, metadata, score}
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ save(index_dir: str)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Save FAISS index to disk
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Save documents to JSON/pickle
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Save BM25 corpus metadata
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ load(index_dir: str) -> bool
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Load FAISS index from disk
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Load stored documents
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Rebuild BM25 ranker
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ filter_by_type(results, metadata_key, value)
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ Filter search results by metadata field
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ Indexing Strategy:
â”‚   â”‚   â”‚       â”œâ”€â”€ FAISS: Flat L2 distance (no compression)
â”‚   â”‚   â”‚       â”œâ”€â”€ BM25: TF-IDF variant for sparse retrieval
â”‚   â”‚   â”‚       â””â”€â”€ Hybrid: Weighted combination of both scores
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ generator.py            # LLM ANSWER GENERATION (Ollama)
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ Generator class
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__(model_name: str = "llama3.2:3b", 
â”‚   â”‚   â”‚   â”‚   â”‚             base_url: str = "http://localhost:11434")
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Initialize Ollama client
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ generate(query: str, context_chunks: List[str], 
â”‚   â”‚   â”‚   â”‚                 num_ctx: int = 2048) -> str
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ Input: User query, list of context chunks
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ Process:
â”‚   â”‚   â”‚   â”‚       â”‚  1. Concatenate context chunks
â”‚   â”‚   â”‚   â”‚       â”‚  2. Build prompt with context + question
â”‚   â”‚   â”‚   â”‚       â”‚  3. Call Ollama API for generation
â”‚   â”‚   â”‚   â”‚       â”‚  4. Stream and assemble response
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ Output: Complete generated answer string
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ Model Configuration:
â”‚   â”‚   â”‚       â”œâ”€â”€ Default Model: llama3.2:3b (3B parameters, 8GB RAM)
â”‚   â”‚   â”‚       â”œâ”€â”€ Alternative Models: llama2:7b, mistral:7b, neural-chat:7b
â”‚   â”‚   â”‚       â”œâ”€â”€ Context Window: 2048 tokens (configurable)
â”‚   â”‚   â”‚       â””â”€â”€ Temperature: 0.7 (configurable for creativity vs accuracy)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ pipeline.py             # RAG PIPELINE ORCHESTRATION
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ RAGPipeline class
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__()
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Initialize Embedder
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Initialize Generator
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Initialize Retriever
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Attempt to load existing index
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Setup logging (logs/queries.jsonl)
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ build_index(pdf_path: str)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: Path to single PDF or directory
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process:
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  1. Load & chunk PDF (data_loader)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  2. Embed chunks (embedder)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  3. Add to index (retriever)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  4. Persist index to disk
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Logging: Record ingestion timestamp & file size
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ query(query_text: str, k: int = 3) -> List[Dict]
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: Query string, number of results
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process:
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  1. Retrieve top-k chunks (retriever.search)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  2. Log query to queries.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  3. Attach metadata (timestamp, chunk_count)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Output: List of {text, metadata, score}
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ask(query_text: str, k: int = 3) -> Dict
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: Query string, context chunk count
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process:
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  1. Retrieve context: query(query_text, k)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  2. Generate answer: generator.generate(query, context)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  3. Assemble response with sources
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Output: {answer: str, sources: List[Dict], 
â”‚   â”‚   â”‚   â”‚   â”‚             metadata: {retrieval_time, generation_time}}
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ State Persistence:
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ Index Directory: indices/ (FAISS + documents)
â”‚   â”‚   â”‚       â”‚       â””â”€â”€ Query Log: logs/queries.jsonl
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ Workflow Diagram:
â”‚   â”‚   â”‚       PDF Input â†’ Load & Chunk â†’ Embed â†’ FAISS/BM25 Index
â”‚   â”‚   â”‚       User Query â†’ Retrieve â†’ Generate â†’ Return with Sources
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ verifier.py             # CODE VERIFICATION UTILITY (Optional)
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ ArchitectureVerifier class
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__(safe_mode: bool = True)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Enable/disable code execution sandbox
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ extract_code_blocks(text: str) -> List[str]
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: Markdown text from LLM response
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process: Parse ```python``` code fences
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Output: List of code block strings
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ verify_dimensions(code: str) -> Dict
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Input: Python code string
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Process: Execute code, capture tensor shapes
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Output: {success: bool, shapes: Dict, errors: str}
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Purpose: Validate ML code correctness
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ __pycache__/            # Python bytecode cache (auto-generated)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ agents/                  # AGENT FRAMEWORK (Future Extension)
â”‚   â”‚   â””â”€â”€ [Empty - Planned for multi-step reasoning agents]
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ knowledge-graph/         # KNOWLEDGE GRAPH (Future Extension)
â”‚       â””â”€â”€ [Empty - Planned for semantic relationship extraction]
â”‚
â”œâ”€â”€ ğŸ“ scripts/                     # UTILITY SCRIPTS
â”‚   â”‚
â”‚   â”œâ”€â”€ download_data.py            # DOWNLOAD KNOWLEDGE BASE PDFS
â”‚   â”‚   â”œâ”€â”€ Purpose: Populate data/ directory with ML PDFs
â”‚   â”‚   â”œâ”€â”€ Functions:
â”‚   â”‚   â”‚   â”œâ”€â”€ download_fundamentals()
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Downloads to data/01_fundamentals/
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ Linear Algebra for Machine Learning (Part 1)
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ download_papers()
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Downloads to data/02_papers/
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ "Attention Is All You Need" (Transformer)
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ "Deep Residual Learning for Image Recognition" (ResNet)
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ "Adam: A Method for Stochastic Optimization"
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ "Denoising Diffusion Probabilistic Models" (DDPM)
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ "Dropout: A Simple Way to Prevent Neural Networks..."
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ download_implementations()
â”‚   â”‚   â”‚       â””â”€â”€ Downloads to data/03_implementation/
â”‚   â”‚   â”‚           â”œâ”€â”€ Deep Learning with PyTorch (Book)
â”‚   â”‚   â”‚           â””â”€â”€ The Little Book of Deep Learning
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ Execution: `python scripts/download_data.py`
â”‚   â”‚
â”‚   â””â”€â”€ ingest_batch.py             # BATCH PDF INGESTION
â”‚       â”œâ”€â”€ Purpose: Process all PDFs in data/ directory
â”‚       â”œâ”€â”€ Process:
â”‚       â”‚   1. Scan data/01_fundamentals/, data/02_papers/, data/03_implementation/
â”‚       â”‚   2. For each PDF:
â”‚       â”‚   â”‚  â””â”€â”€ Load â†’ Chunk â†’ Embed â†’ Add to index
â”‚       â”‚   3. Save consolidated FAISS index to indices/
â”‚       â”‚   4. Log ingestion results
â”‚       â”‚
â”‚       â””â”€â”€ Execution: `python scripts/ingest_batch.py`
â”‚
â”œâ”€â”€ ğŸ“ tests/                       # UNIT & INTEGRATION TESTS
â”‚   â”‚
â”‚   â”œâ”€â”€ test_data_loader.py         # DATA LOADING TESTS
â”‚   â”‚   â”œâ”€â”€ test_paragraph_splitting()
â”‚   â”‚   â”‚   â””â”€â”€ Verify text chunking respects paragraphs
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ test_chunking_limit()
â”‚   â”‚   â”‚   â””â”€â”€ Verify chunk sizes don't exceed limit
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ test_overlap()
â”‚   â”‚   â”‚   â””â”€â”€ Verify overlap ratio applied correctly
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ test_smart_behavior()
â”‚   â”‚       â””â”€â”€ Verify metadata attached to chunks
â”‚   â”‚
â”‚   â”œâ”€â”€ test_hybrid_search.py       # RETRIEVAL TESTS
â”‚   â”‚   â”œâ”€â”€ test_retriever_hybrid_integration()
â”‚   â”‚   â”‚   â””â”€â”€ Verify FAISS + BM25 hybrid search works
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ test_save_load()
â”‚   â”‚       â””â”€â”€ Verify index persistence/loading
â”‚   â”‚
â”‚   â””â”€â”€ __pycache__/                # Python bytecode cache
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                   # JUPYTER NOTEBOOKS (Experimental/Dev)
â”‚   â”‚
â”‚   â”œâ”€â”€ 1-text-extraction.ipynb     # PDF EXTRACTION EXPERIMENTS
â”‚   â”‚   â”œâ”€â”€ Cell 1: Import libraries
â”‚   â”‚   â”œâ”€â”€ Cell 2: Load PDF with marker-pdf
â”‚   â”‚   â”œâ”€â”€ Cell 3: Extract text page-by-page
â”‚   â”‚   â””â”€â”€ Cell 4: Visualize text quality
â”‚   â”‚
â”‚   â”œâ”€â”€ 2-embedding.ipynb           # EMBEDDING EXPERIMENTS
â”‚   â”‚   â”œâ”€â”€ Cell 1: Load SentenceTransformer
â”‚   â”‚   â”œâ”€â”€ Cell 2: Embed sample texts
â”‚   â”‚   â”œâ”€â”€ Cell 3: Compute cosine similarity
â”‚   â”‚   â””â”€â”€ Cell 4: Visualize embeddings (t-SNE/UMAP)
â”‚   â”‚
â”‚   â””â”€â”€ [Root-level duplicates for quick access]
â”‚       â”œâ”€â”€ 1-text-extraction.ipynb
â”‚       â””â”€â”€ 2-embedding.ipynb
â”‚
â”œâ”€â”€ ğŸ“ data/                        # DATA & KNOWLEDGE BASE
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“„ sample.pdf               # Sample test PDF
â”‚   â”œâ”€â”€ ğŸ“„ terminal.pdf             # Terminal/shell concepts PDF
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ chunks/                  # PROCESSED TEXT CHUNKS
â”‚   â”‚   â””â”€â”€ terminal_chunks.jsonl   # Chunked text from terminal.pdf
â”‚   â”‚       â””â”€â”€ Format: One JSON object per line
â”‚   â”‚           â””â”€â”€ {text: str, metadata: {page: int, source: str, chunk_idx: int}}
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ 01_fundamentals/         # FUNDAMENTAL ML RESOURCES
â”‚   â”‚   â””â”€â”€ linear_algebra_for_ml_part1.pdf
â”‚   â”‚       â””â”€â”€ Topics: Vectors, matrices, decomposition
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ 02_papers/               # SEMINAL RESEARCH PAPERS
â”‚   â”‚   â”œâ”€â”€ attention_is_all_you_need.pdf
â”‚   â”‚   â”‚   â””â”€â”€ Transformers, multi-head attention
â”‚   â”‚   â”œâ”€â”€ resnet.pdf
â”‚   â”‚   â”‚   â””â”€â”€ Residual networks, skip connections
â”‚   â”‚   â”œâ”€â”€ adam_optimizer.pdf
â”‚   â”‚   â”‚   â””â”€â”€ Adaptive learning rates for optimization
â”‚   â”‚   â”œâ”€â”€ ddpm_diffusion.pdf
â”‚   â”‚   â”‚   â””â”€â”€ Denoising diffusion probabilistic models
â”‚   â”‚   â””â”€â”€ dropout_srivastava14a.pdf
â”‚   â”‚       â””â”€â”€ Regularization technique for neural networks
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ 03_implementation/       # IMPLEMENTATION GUIDES
â”‚       â”œâ”€â”€ deep_learning_with_pytorch.pdf
â”‚       â”‚   â””â”€â”€ PyTorch fundamentals, training loops, models
â”‚       â””â”€â”€ the_little_book_of_deep_learning.pdf
â”‚           â””â”€â”€ Deep learning principles, architectures, best practices
â”‚
â”œâ”€â”€ ğŸ“ indices/                     # PERSISTED SEARCH INDICES (Generated)
â”‚   â”œâ”€â”€ faiss.index                 # FAISS vector database
â”‚   â”‚   â””â”€â”€ Contains embeddings for all chunks
â”‚   â”œâ”€â”€ documents.json              # Original chunk documents + metadata
â”‚   â”‚   â””â”€â”€ Array of {text, metadata} objects
â”‚   â””â”€â”€ bm25_metadata.pkl           # BM25 corpus metadata (pickle)
â”‚
â”œâ”€â”€ ğŸ“ logs/                        # QUERY LOGS & ANALYTICS
â”‚   â”‚
â”‚   â”œâ”€â”€ queries.jsonl               # QUERY HISTORY LOG
â”‚   â”‚   â”œâ”€â”€ Format: One JSON object per line
â”‚   â”‚   â”œâ”€â”€ Fields: {
â”‚   â”‚   â”‚     timestamp: str (ISO 8601),
â”‚   â”‚   â”‚     query: str,
â”‚   â”‚   â”‚     retrieval_time: float (seconds),
â”‚   â”‚   â”‚     generation_time: float (seconds),
â”‚   â”‚   â”‚     num_chunks_retrieved: int,
â”‚   â”‚   â”‚     model_used: str
â”‚   â”‚   â”‚   }
â”‚   â”‚   â””â”€â”€ Purpose: Track query patterns, performance, user interactions
â”‚   â”‚
â”‚   â””â”€â”€ [Additional logs]: errors.log, warnings.log (optional)
â”‚
â”œâ”€â”€ ğŸ“ backend/                     # BACKEND API (Future)
â”‚   â””â”€â”€ [Empty - Planned for FastAPI/Flask REST API]
â”‚
â”œâ”€â”€ ğŸ“ frontend/                    # FRONTEND UI (Future)
â”‚   â””â”€â”€ [Empty - Planned for React/Vue web interface]
â”‚
â”œâ”€â”€ ğŸ“ docs/                        # DOCUMENTATION (Future)
â”‚   â””â”€â”€ [Empty - Planned for API docs, guides, examples]
â”‚
â”œâ”€â”€ .vscode/                        # VS CODE SETTINGS
â”‚   â”œâ”€â”€ launch.json                 # Debugger configuration
â”‚   â”œâ”€â”€ settings.json               # Editor settings
â”‚   â””â”€â”€ extensions.json             # Recommended extensions
â”‚
â”œâ”€â”€ .ipynb_checkpoints/             # Jupyter checkpoints (auto-generated)
â”‚
â”œâ”€â”€ .pytest_cache/                  # Pytest cache (auto-generated)
â”‚
â”œâ”€â”€ .git/                           # Git repository metadata
â”‚
â””â”€â”€ System Volume Information/      # Windows system folder (ignore)

```

---

## ğŸ”— Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERACTION                             â”‚
â”‚  CLI (main.py) â†’ Interactive REPL / --ingest / --query              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ INDEX BUILD  â”‚              â”‚  QUERY EXECUTION â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                              â”‚
          â–¼                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ data_loader.py       â”‚      â”‚ retriever.search()   â”‚
    â”‚ (PDF â†’ Chunks)       â”‚      â”‚ (FAISS + BM25)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                             â”‚
               â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ embedder.encode()    â”‚      â”‚ Retrieved Chunks     â”‚
    â”‚ (Text â†’ Embeddings)  â”‚      â”‚ (Top-k Results)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                             â”‚
               â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ retriever.add()      â”‚      â”‚ generator.generate() â”‚
    â”‚ (Build Indices)      â”‚      â”‚ (LLM â†’ Answer)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                             â”‚
               â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ retriever.save()     â”‚      â”‚ Format Response      â”‚
    â”‚ (Persist Indices)    â”‚      â”‚ (Answer + Sources)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                             â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Output to User   â”‚
                        â”‚ (CLI Display)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Component Interaction Details

### 1. **Data Ingestion Pipeline**
```
PDF File
  â†“ data_loader.load_and_chunk_pdf()
  â”œâ”€ Extract text + metadata (page, source)
  â”œâ”€ Split into chunks (smart word-based)
  â””â”€ Return: List[{text, metadata}]
    â†“ pipeline.build_index()
    â”œâ”€ Pass chunks to embedder.encode()
    â”œâ”€ embedder calls SentenceTransformer
    â””â”€ Returns: List[embedding_vector]
      â†“ retriever.add()
      â”œâ”€ Add to FAISS index (flat L2)
      â”œâ”€ Build BM25 corpus
      â””â”€ Persist: indices/faiss.index, indices/documents.json
```

### 2. **Query & Retrieval Pipeline**
```
User Query (string)
  â†“ pipeline.query(query_text, k=3)
  â”œâ”€ embedder.encode_single(query_text)
  â”‚  â””â”€ Returns: 1D embedding vector
  â”œâ”€ retriever.search(query_text, k=3)
  â”‚  â”œâ”€ FAISS search: L2 distance â†’ top-3 semantic matches
  â”‚  â”œâ”€ BM25 search: TF-IDF â†’ top-3 keyword matches
  â”‚  â””â”€ Merge & rank: Combined hybrid score
  â”œâ”€ Log to logs/queries.jsonl
  â””â”€ Return: List[{text, metadata, score}]
```

### 3. **Answer Generation Pipeline**
```
(Query, Retrieved Chunks)
  â†“ pipeline.ask(query, context_chunks)
  â”œâ”€ Concatenate chunk texts
  â”œâ”€ Build prompt: "Context: ...\n\nQuestion: {query}\n\nAnswer:"
  â”œâ”€ generator.generate(query, chunks)
  â”‚  â”œâ”€ Call Ollama API
  â”‚  â”œâ”€ Stream response tokens
  â”‚  â””â”€ Assemble full answer
  â”œâ”€ Format response dict:
  â”‚  â”œâ”€ answer: str
  â”‚  â”œâ”€ sources: List[{text, metadata}]
  â”‚  â””â”€ metadata: {retrieval_time, generation_time}
  â””â”€ Return: Response dict
    â†“ Display to user with formatted output
```

---

## ğŸ“¦ Dependencies & Requirements

### Core Dependencies
```
ollama              # Local LLM inference client
faiss-cpu          # Vector similarity search (CPU-only)
sentence-transformers  # Text embeddings
marker-pdf         # High-quality PDF text extraction
numpy              # Numerical computations
rank-bm25          # BM25 keyword ranking
python-dotenv      # Environment variable management
```

### Development Dependencies
```
pytest             # Unit testing framework
jupyter            # Interactive notebooks
```

### System Requirements
- **Python**: 3.8+ (tested on 3.10)
- **RAM**: 8+ GB (for Ollama model + FAISS index)
- **GPU**: Optional (CUDA for faster embeddings)
- **Ollama**: Installed and running on localhost:11434

---

## ğŸš€ Usage Workflows

### Setup & Installation
```bash
# Clone repository
git clone <repo-url>
cd ai-knowledge-assistant

# Create virtual environment (conda)
conda create -n rag python=3.10
conda activate rag

# Install dependencies
pip install -r requirements.txt

# Download Ollama model
ollama pull llama3.2:3b

# Download knowledge base PDFs
python scripts/download_data.py

# Ingest PDFs into index
python scripts/ingest_batch.py
```

### Usage Modes

#### 1. Interactive Mode
```bash
python main.py
# Type queries and get answers with source attribution
```

#### 2. Single Query Mode
```bash
python main.py --query "What is attention mechanism?"
```

#### 3. Ingest + Query
```bash
python main.py --ingest data/sample.pdf --query "What is transformer?"
```

#### 4. Batch Ingestion
```bash
python scripts/ingest_batch.py
```

#### 5. Testing
```bash
pytest tests/
```

#### 6. Notebook Exploration
```bash
jupyter notebook notebooks/
# Open 1-text-extraction.ipynb or 2-embedding.ipynb
```

---

## ğŸ”§ Configuration & Extensibility

### Environment Variables (local.env)
```bash
OLLAMA_MODEL=llama3.2:3b           # LLM model name
OLLAMA_BASE_URL=http://localhost:11434  # Ollama server URL
EMBEDDING_MODEL=all-MiniLM-L6-v2   # SentenceTransformer model
CHUNK_SIZE=256                     # Text chunk size (words)
CHUNK_OVERLAP=0.1                  # Overlap ratio (0.1 = 10%)
CONTEXT_WINDOW=2048                # LLM context window (tokens)
LOG_LEVEL=INFO                     # Logging level
```

### Customization Points

**Change LLM Model**:
```python
# In src/rag/generator.py
generator = Generator(model_name="mistral:7b")
```

**Change Embedding Model**:
```python
# In src/rag/pipeline.py
embedder = Embedder(model_name="sentence-transformers/all-mpnet-base-v2")
```

**Adjust Chunking Parameters**:
```python
# In src/rag/pipeline.py
pipeline.build_index(pdf_path, chunk_size=512, overlap_ratio=0.2)
```

**Change Search Strategy**:
```python
# In src/rag/retriever.py
results = retriever.search(query, k=10)  # Return top-10 instead of 5
```

---

## ğŸ“Š Performance Characteristics

| Operation | Time | Notes |
|-----------|------|-------|
| PDF Ingestion (10 pages) | 5-10s | Includes chunking + embedding |
| Embedding Single Text | ~20ms | CPU inference, varies by text length |
| FAISS Search (1M vectors) | ~5ms | L2 distance, flat index |
| BM25 Search | ~10ms | TF-IDF ranking |
| Hybrid Search | ~15ms | Combined FAISS + BM25 |
| LLM Generation | 5-30s | Depends on output length + model |
| Full RAG Query | 10-40s | Retrieve + Generate |

---

## ğŸ§ª Testing Strategy

### Unit Tests
- **test_data_loader.py**: Chunking logic, metadata preservation
- **test_hybrid_search.py**: Index persistence, hybrid ranking

### Integration Tests
- End-to-end PDF ingestion + query + generation

### Manual Testing
- Interactive CLI testing
- Verify source attribution accuracy
- Check generation quality on test queries

---

## ğŸ”® Future Extensions

### Planned Features
1. **Backend API**: FastAPI REST endpoints
2. **Frontend UI**: React/Vue web interface
3. **Agents Framework**: Multi-step reasoning agents
4. **Knowledge Graph**: Automatic relationship extraction
5. **Advanced Retrieval**: MMR (Maximal Marginal Relevance), re-ranking
6. **Multi-Modal**: Support for images in PDFs
7. **Streaming UI**: Real-time token streaming to client
8. **Fine-tuning**: Custom embeddings/LLM fine-tuning
9. **Monitoring**: Query performance analytics, user feedback loops
10. **Caching**: Query result caching for common questions

---

## ğŸ“ Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| "No module named 'faiss'" | FAISS not installed | `pip install faiss-cpu` |
| "Connection refused to Ollama" | Ollama not running | `ollama serve` in separate terminal |
| "CUDA out of memory" | GPU memory exceeded | Use CPU embeddings: remove CUDA |
| "PDF extraction failed" | Corrupted PDF or unsupported format | Try with sample.pdf |
| "Index not found" | No ingestion completed | Run `python scripts/ingest_batch.py` |
| "Slow query response" | Large context window or slow LLM | Reduce `k` parameter or use faster model |

---

## ğŸ“„ License & Attribution

- **Framework**: LLaMA 2 / Llama 3.2
- **Embeddings**: Hugging Face Sentence Transformers
- **Vector DB**: Meta FAISS
- **PDF Extraction**: Unified-IO Marker-PDF
- **BM25**: Rank-bm25 library

---

**Last Updated**: February 7, 2026
**Version**: 1.0 (Production Ready)
**Maintainers**: AI Knowledge Assistant Team
